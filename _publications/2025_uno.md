---
title: "UNO: Towards Unified Baseline for Video Scene Graph Generation via Object-Centric Representation Learning"
image: "/images/publications/uno_2025.png"
arxiv: 
github: 
authors: 
    - huyle
    - nhatchung
    - tungkieu
    - jingkang
    - nganle
collection: publications
permalink: /publication/uno
type: "under review"
publication: "Under Review"
year: "2025"
date: 2025-02-12
---
<!-- <button class="btn btn-round btn-sm btn-ghost-blue" onclick="location.href='https://arxiv.org/abs/2312.09507'">arXiv</button> -->

## Abstract
Video scene graph generation (VidSGG) aims to construct dynamic scene graphs that represent videos by decomposing them into individual objects and their pair-wise relationships. Existing research in VidSGG has diverged into two task-specific frameworks, i.e. coarse-grained box-level and fine-grained panoptic mask-level VidSGG. Building upon this divergence, our research aims to develop a unified framework designed to minimize task-specific modifications and multi-stage training/inference while maximizing shared parameters, and generalizing to varied visual granularity. The primary challenge in achieving that goal lies in maintaining a unified spatio-temporal representation that aligns with video dynamics across both tasks. Thus, we introduce UNO (UNified Object-centric VidSGG), a one-stage, object-centric framework that handles both box-level and mask-level VidSGG. The key aspect of UNO’s design is an extended slot attention mechanism, which learns to decompose visual features into modular objects and relation slots. As a one-stage strategy without additional tracking modules, UNO incorporates object temporal consistency learning on the extracted object slots, enabling consistent object tracking over time. To directly associate relation slots with their corresponding object slots, we introduce a dynamic triplet prediction module. We have evaluated the effectiveness and efficiency of our UNO on both box-level VidSGG using the Action Genome dataset and panoptic mask-level VidSGG using the PVSG dataset.
<!-- 
## Citation
Le, H., Kieu, T., Nguyen, A., and Le, N., “WAVER: Writing-style Agnostic Text-Video Retrieval via Distilling Vision-Language Models through Open-Vocabulary Knowledge”, <i>Under Review</i>, 2024. -->