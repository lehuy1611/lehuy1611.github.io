---
title: "Tracked-Vehicle Retrieval by Natural Language Descriptions With Multi-Contextual Adaptive Knowledge"
image: "/images/publications/aictrack2_2023.png"
link: https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Le_Tracked-Vehicle_Retrieval_by_Natural_Language_Descriptions_With_Multi-Contextual_Adaptive_Knowledge_CVPRW_2023_paper.html
github: https://github.com/zef1611/AIC23_NLRetrieval_HCMIU_CVIP
authors: "<b>Huy Le</b>, Quang Nguyen, Duc Luu, Truc Chau, Nhat Chung, Synh Ha"
collection: publications
permalink: /publication/aictrack2_2023
type: "workshop paper"
publication: "<b>CVPR</b> Workshop"
year: "2023"
date: 2023-12-04
---

## Abstract
Text-video retrieval, a prominent sub-field within the domain of multimodal information retrieval, has witnessed remarkable growth in recent years. However, existing methods assume video scenes are consistent with unbiased descriptions. These limitations fail to align with real-world scenarios since descriptions can be influenced by annotator biases, diverse writing styles, and varying textual perspectives. To overcome the aforementioned problems, we introduce ğš†ğ™°ğš…ğ™´ğš, a cross-domain knowledge distillation framework via vision-language models through open-vocabulary knowledge designed to tackle the challenge of handling different writing styles in video descriptions. ğš†ğ™°ğš…ğ™´ğš capitalizes on the open-vocabulary properties that lie in pre-trained vision-language models and employs an implicit knowledge distillation approach to transfer text-based knowledge from a teacher model to a vision-based student. Empirical studies conducted across four standard benchmark datasets, encompassing various settings, provide compelling evidence that ğš†ğ™°ğš…ğ™´ğš can achieve state-of-the-art performance in text-video retrieval task while handling writing-style variations.

## Citation
Le, H., Kieu, T., Nguyen, A., and Le, N., â€œWAVER: Writing-style Agnostic Text-Video Retrieval via Distilling Vision-Language Models through Open-Vocabulary Knowledgeâ€, <i>ICASSP</i>, 2024.